{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 4.,  8.])]\n"
     ]
    }
   ],
   "source": [
    "# feedforward_example.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Example of training a feedforward network with one hidden layer to solve XOR.\n",
    "if __name__==\"__main__\":\n",
    "    # MAKE THE DATA\n",
    "    # Synthetic data for XOR: y = x0 XOR x1\n",
    "    train_xs = np.array([[[1.0, 1.0], [1.0, 1.0]],[[2.0, 2.0], [2.0, 2.0]]])\n",
    "    train_ys = np.array([[1.0, 1.0],[1.0, 1.0]])\n",
    "    x = tf.constant(train_xs)\n",
    "    t = tf.constant(train_ys)\n",
    "    out = tf.tensordot(x, t, 2)\n",
    "    sess = tf.Session()\n",
    "    print(sess.run([out]))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sentiment_data' from 'sentiment_data.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(md)\n",
    "reload(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 17614 vectors of size 300\n",
      "8530 / 1066 / 1066 train/dev/test examples\n",
      "(8530, 300)\n",
      "<type 'numpy.float64'>\n",
      "(1066, 300)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import models as md\n",
    "import sentiment_data as sd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use either 50-dim or 300-dim vectors\n",
    "    #word_vectors = read_word_embeddings(\"data/glove.6B.50d-relativized.txt\")\n",
    "    word_vectors = sd.read_word_embeddings(\"data/glove.6B.300d-relativized.txt\")\n",
    "\n",
    "    # Load train, dev, and test exs\n",
    "    train_exs = sd.read_and_index_sentiment_examples(\"data/train.txt\", word_vectors.word_indexer)\n",
    "    dev_exs = sd.read_and_index_sentiment_examples(\"data/dev.txt\", word_vectors.word_indexer)\n",
    "    test_exs = sd.read_and_index_sentiment_examples(\"data/test-blind.txt\", word_vectors.word_indexer)\n",
    "    print repr(len(train_exs)) + \" / \" + repr(len(dev_exs)) + \" / \" + repr(len(test_exs)) + \" train/dev/test examples\"\n",
    "\n",
    "    system_to_run = \"FF\"\n",
    "    \n",
    "    if system_to_run == \"FF\":\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            test_exs_predicted = md.train_ffnn(train_exs, dev_exs, test_exs, word_vectors, 30)\n",
    "#         write_sentiment_examples(test_exs_predicted, \"test-blind.output.txt\", word_vectors.word_indexer)\n",
    "    elif system_to_run == \"FANCY\":\n",
    "        test_exs_predicted = md.train_fancy(train_exs[:100], dev_exs, test_exs, word_vectors)\n",
    "    else:\n",
    "        raise Exception(\"Pass in either FF or FANCY to run the appropriate system\")\n",
    "    # Write the test set output\n",
    "#     write_sentiment_examples(test_exs_predicted, \"test-blind.output.txt\", word_vectors.word_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 17614 vectors of size 300\n",
      "8530 / 1066 / 1066 train/dev/test examples\n",
      "(100, 60, 300)\n",
      "<type 'numpy.ndarray'>\n",
      "(1066, 60, 300)\n",
      "Loss for iteration 0: 71.334332042967219\n",
      "596/1066 correct after training\n",
      "0.5590994371482176 correct after training\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './output/aquila/bi-lstm-2_e0_a0.559.output.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea466fb90b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msystem_to_run\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"FANCY\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtest_exs_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_bi_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_exs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_exs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_exs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aquila/bi-lstm-2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pass in either FF or FANCY to run the appropriate system\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/v/filer5b/v20q001/prateekk/projects/nlp_proj3_senti/src/models.pyc\u001b[0m in \u001b[0;36mtrain_bi_lstm\u001b[0;34m(train_exs, dev_exs, test_exs, word_vectors, file_desc, num_epochs)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mtest_exs_predicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_xs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_seq_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_exs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mwrite_sentiment_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_exs_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./output/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_desc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_e\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_a\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".output.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_indexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Total Time:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/v/filer5b/v20q001/prateekk/projects/nlp_proj3_senti/src/sentiment_data.pyc\u001b[0m in \u001b[0;36mwrite_sentiment_examples\u001b[0;34m(exs, outfile, indexer)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# out is tokenized and contains UNKs, so this will not exactly match the input file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_sentiment_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexed_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './output/aquila/bi-lstm-2_e0_a0.559.output.txt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import models as md\n",
    "import sentiment_data as sd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use either 50-dim or 300-dim vectors\n",
    "    #word_vectors = read_word_embeddings(\"data/glove.6B.50d-relativized.txt\")\n",
    "    word_vectors = sd.read_word_embeddings(\"data/glove.6B.300d-relativized.txt\")\n",
    "\n",
    "    # Load train, dev, and test exs\n",
    "    train_exs = sd.read_and_index_sentiment_examples(\"data/train.txt\", word_vectors.word_indexer)\n",
    "    dev_exs = sd.read_and_index_sentiment_examples(\"data/dev.txt\", word_vectors.word_indexer)\n",
    "    test_exs = sd.read_and_index_sentiment_examples(\"data/test-blind.txt\", word_vectors.word_indexer)\n",
    "    print repr(len(train_exs)) + \" / \" + repr(len(dev_exs)) + \" / \" + repr(len(test_exs)) + \" train/dev/test examples\"\n",
    "\n",
    "    system_to_run = \"FANCY\"\n",
    "    \n",
    "    if system_to_run == \"FF\":\n",
    "        with tf.variable_scope(\"model\", reuse=None):\n",
    "            test_exs_predicted = md.train_ffnn(train_exs, dev_exs, test_exs, word_vectors, 10)\n",
    "#         write_sentiment_examples(test_exs_predicted, \"test-blind.output.txt\", word_vectors.word_indexer)\n",
    "    elif system_to_run == \"FANCY\":\n",
    "        with tf.variable_scope(\"model2\", reuse=None):\n",
    "            test_exs_predicted = md.train_bi_lstm(train_exs, dev_exs, test_exs, word_vectors, \"aquila_bi-lstm-1\", 30)\n",
    "    else:\n",
    "        raise Exception(\"Pass in either FF or FANCY to run the appropriate system\")\n",
    "    # Write the test set output\n",
    "#     write_sentiment_examples(test_exs_predicted, \"test-blind.output.txt\", word_vectors.word_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 17614 vectors of size 300\n",
      "[[1 2]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import models as md\n",
    "import sentiment_data as sd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "word_vectors = sd.read_word_embeddings(\"data/glove.6B.300d-relativized.txt\")\n",
    "train_exs = sd.read_and_index_sentiment_examples(\"data/train.txt\", word_vectors.word_indexer)\n",
    "len(train_exs)\n",
    "\n",
    "a=np.array([[1,2],[3,4]])\n",
    "print a[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = train_exs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = np.zeros([len(examples),word_vectors.vectors.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5c8acdd423b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_from_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_from_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_from_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "for i in xrange(len(examples)):\n",
    "    word_idxs = np.array(examples[i].indexed_words)\n",
    "    all_words = np.array([len(word_idxs), word_vectors.vectors.shape[1]])\n",
    "    for j in xrange(len(word_idxs)):\n",
    "        print len(word_vectors.get_embedding_from_idx(int(word_idxs[j])))\n",
    "        print type(word_vectors.get_embedding_from_idx(int(word_idxs[j])))\n",
    "        all_words[j] = word_vectors.get_embedding_from_idx(int(word_idxs[j]))\n",
    "    xs=all_words[j].mean(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   92,     8,  4152,    55,    34,  1118,  5721,     1,  7267,\n",
       "        1139,    31,    33,  1873,  7272,     1,    13,  2291,   182,\n",
       "       12615,     5, 14552,     0,  3350,  2378,    61,     4,    27,     2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f41e9a8b3f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexed_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_from_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=0\n",
    "word_idxs = np.array(examples[i].indexed_words)\n",
    "all_words = np.array([len(word_idxs), word_vectors.vectors.shape[1]])\n",
    "all_words[j] = word_vectors.get_embedding_from_idx(int(word_idxs[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words.shape\n",
    "word_vectors.vectors.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
